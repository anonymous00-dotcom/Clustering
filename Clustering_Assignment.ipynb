{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Clustering"
      ],
      "metadata": {
        "id": "main_heading"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Theoretical Questions"
      ],
      "metadata": {
        "id": "theory_heading"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Question 1:** What is unsupervised learning in the context of machine learning?"
      ],
      "metadata": {
        "id": "q_1"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Answer:**\n",
        "**Unsupervised learning** is a type of machine learning where the algorithm learns patterns from **unlabeled data**. Unlike supervised learning, there is no predefined target or output variable. The goal is to explore the data and find meaningful structures or patterns on its own, such as grouping similar data points together (clustering) or reducing the number of features (dimensionality reduction)."
      ],
      "metadata": {
        "id": "a_1"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Question 2:** How does K-Means clustering algorithm work?"
      ],
      "metadata": {
        "id": "q_2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Answer:**\n",
        "The **K-Means clustering algorithm** works by partitioning a dataset into a predefined number of 'K' distinct, non-overlapping clusters. The steps are as follows:\n",
        "1. **Initialization:** Randomly select 'K' initial cluster centroids (center points) from the data.\n",
        "2. **Assignment Step:** Assign each data point to the nearest cluster centroid, based on a distance metric (usually Euclidean distance).\n",
        "3. **Update Step:** Recalculate the centroid of each cluster by taking the mean of all data points assigned to it.\n",
        "4. **Iteration:** Repeat the Assignment and Update steps until the centroids no longer move significantly, meaning the clusters have stabilized."
      ],
      "metadata": {
        "id": "a_2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Question 3:** Explain the concept of a dendrogram in hierarchical clustering."
      ],
      "metadata": {
        "id": "q_3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Answer:**\n",
        "A **dendrogram** is a tree-like diagram used to visualize the arrangement of the clusters produced by hierarchical clustering. It illustrates how clusters are merged (in agglomerative clustering) or split (in divisive clustering). The y-axis of the dendrogram represents the distance or dissimilarity between clusters. By cutting the dendrogram at a certain height, you can determine the number of clusters for your dataset."
      ],
      "metadata": {
        "id": "a_3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Question 4:** What is the main difference between K-Means and Hierarchical Clustering?"
      ],
      "metadata": {
        "id": "q_4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Answer:**\n",
        "The main difference is that **K-Means** is a **partitional** clustering algorithm, while **Hierarchical Clustering** is, as the name suggests, **hierarchical**.\n",
        "- In **K-Means**, you must specify the number of clusters (K) beforehand, and it assigns each data point to exactly one cluster.\n",
        "- In **Hierarchical Clustering**, you do not need to pre-specify the number of clusters. It builds a hierarchy of clusters, which can be visualized using a dendrogram, allowing you to choose the number of clusters after the fact."
      ],
      "metadata": {
        "id": "a_4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Question 5:** What are the advantages of DBSCAN over K-Means?"
      ],
      "metadata": {
        "id": "q_5"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Answer:**\n",
        "DBSCAN (Density-Based Spatial Clustering of Applications with Noise) has several key advantages over K-Means:\n",
        "1. **Arbitrary Shaped Clusters:** DBSCAN can find clusters of arbitrary shapes, whereas K-Means assumes that clusters are spherical.\n",
        "2. **No Need to Specify K:** You do not need to pre-specify the number of clusters. The algorithm finds the number of clusters on its own.\n",
        "3. **Outlier Detection:** DBSCAN has a built-in mechanism for identifying noise points (outliers) that do not belong to any cluster."
      ],
      "metadata": {
        "id": "a_5"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Question 6:** When would you use Silhouette Score in clustering?"
      ],
      "metadata": {
        "id": "q_6"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Answer:**\n",
        "The **Silhouette Score** is used to evaluate the quality of clusters created by an algorithm like K-Means. You would use it to determine how well-separated and dense the clusters are. It is particularly useful for:\n",
        "- **Choosing the Optimal Number of Clusters (K):** You can calculate the Silhouette Score for different values of K and choose the K that yields the highest score.\n",
        "- **Comparing Different Clustering Algorithms:** It can be used to compare the performance of different clustering algorithms on the same dataset."
      ],
      "metadata": {
        "id": "a_6"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Question 7:** What are the limitations of Hierarchical Clustering?"
      ],
      "metadata": {
        "id": "q_7"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Answer:**\n",
        "The main limitations of Hierarchical Clustering are:\n",
        "- **High Computational Complexity:** It is computationally expensive, typically with a time complexity of O(n^3) and space complexity of O(n^2), making it unsuitable for very large datasets.\n",
        "- **Irreversible Decisions:** Once a merge or split is made, it cannot be undone. An early incorrect decision can lead to poor final clusters.\n",
        "- **Sensitivity to Noise:** It can be sensitive to noise and outliers in the data."
      ],
      "metadata": {
        "id": "a_7"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Question 8:** Why is feature scaling important in clustering algorithms like K-Means?"
      ],
      "metadata": {
        "id": "q_8"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Answer:**\n",
        "Feature scaling is crucial because K-Means is a **distance-based algorithm**. If features are on different scales (e.g., age in years vs. income in thousands), the feature with the larger range will dominate the distance calculation. This means the clusters will be biased towards that feature. Scaling the features (e.g., using `StandardScaler` or `MinMaxScaler`) ensures that all features contribute equally to the distance computation, leading to more meaningful and accurate clusters."
      ],
      "metadata": {
        "id": "a_8"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Question 9:** How does DBSCAN identify noise points?"
      ],
      "metadata": {
        "id": "q_9"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Answer:**\n",
        "DBSCAN identifies noise points based on its concept of density. It classifies data points into three types:\n",
        "- **Core points:** Points that have at least a minimum number of other points (`MinPts`) within a specified radius (`eps`).\n",
        "- **Border points:** Points that are within the radius of a core point but do not have enough neighbors to be core points themselves.\n",
        "- **Noise points (outliers):** Any point that is neither a core point nor a border point. These are points that are isolated in low-density regions."
      ],
      "metadata": {
        "id": "a_9"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Question 10:** Define inertia in the context of K-Means."
      ],
      "metadata": {
        "id": "q_10"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Answer:**\n",
        "**Inertia** in K-Means is the **sum of squared distances** of samples to their closest cluster center. It measures how internally coherent the clusters are. A lower inertia value means that the clusters are more dense and well-defined. The K-Means algorithm aims to find the cluster centers that minimize this inertia value."
      ],
      "metadata": {
        "id": "a_10"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Question 11:** What is the elbow method in K-Means clustering?"
      ],
      "metadata": {
        "id": "q_11"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Answer:**\n",
        "The **elbow method** is a heuristic used to determine the optimal number of clusters (K) in a K-Means algorithm. It works by plotting the inertia (sum of squared distances) for a range of K values. As K increases, inertia decreases. The plot typically looks like an arm, and the \"elbow\" point—where the rate of decrease in inertia sharply slows down—is considered to be the optimal value for K."
      ],
      "metadata": {
        "id": "a_11"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Question 12:** Describe the concept of \"density\" in DBSCAN."
      ],
      "metadata": {
        "id": "q_12"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Answer:**\n",
        "In DBSCAN, \"density\" at a particular point is defined by two parameters: `eps` (epsilon) and `MinPts` (minimum points). A region is considered **dense** if it contains at least `MinPts` number of points within a radius of `eps`. This concept of density allows the algorithm to form clusters by connecting dense regions and to identify points in sparse regions as noise."
      ],
      "metadata": {
        "id": "a_12"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Question 13:** Can hierarchical clustering be used on categorical data?"
      ],
      "metadata": {
        "id": "q_13"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Answer:**\n",
        "Yes, hierarchical clustering can be used on categorical data, but it requires an appropriate dissimilarity measure. Standard distance metrics like Euclidean distance are not suitable for categorical features. Instead, you would need to use a metric like the **Jaccard distance** or create a dissimilarity matrix based on techniques like Gower's distance, which can handle mixed data types."
      ],
      "metadata": {
        "id": "a_13"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Question 14:** What does a negative Silhouette Score indicate?"
      ],
      "metadata": {
        "id": "q_14"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Answer:**\n",
        "A negative Silhouette Score for a data point indicates that it has likely been **assigned to the wrong cluster**. It means that the average distance to the points in its own cluster is greater than the average distance to the points in the nearest neighboring cluster. This suggests that the point is closer to a different cluster than the one it was assigned to."
      ],
      "metadata": {
        "id": "a_14"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Question 15:** Explain the term \"linkage criteria\" in hierarchical clustering."
      ],
      "metadata": {
        "id": "q_15"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Answer:**\n",
        "**Linkage criteria** defines how the distance between two clusters is measured in agglomerative hierarchical clustering. This is crucial for deciding which clusters to merge at each step. Common linkage criteria include:\n",
        "- **Single Linkage:** The distance between the closest points in the two clusters.\n",
        "- **Complete Linkage:** The distance between the farthest points in the two clusters.\n",
        "- **Average Linkage:** The average distance between all pairs of points in the two clusters.\n",
        "- **Ward's Linkage:** Minimizes the increase in variance after merging clusters."
      ],
      "metadata": {
        "id": "a_15"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Question 16:** Why might K-Means clustering perform poorly on data with varying cluster sizes or densities?"
      ],
      "metadata": {
        "id": "q_16"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Answer:**\n",
        "K-Means performs poorly in such scenarios because it implicitly assumes that clusters are **spherical and have similar sizes and densities**. Its objective is to minimize inertia, which works best when clusters are well-separated and roughly equal in size. When faced with varying densities or sizes, K-Means can struggle to place centroids correctly, often splitting large or sparse clusters and incorrectly grouping smaller, denser ones."
      ],
      "metadata": {
        "id": "a_16"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Question 17:** What are the core parameters in DBSCAN, and how do they influence clustering?"
      ],
      "metadata": {
        "id": "q_17"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Answer:**\n",
        "The two core parameters in DBSCAN are:\n",
        "1. **`eps` (epsilon):** This is the radius that defines the neighborhood around a data point. It determines how close points must be to each other to be considered part of the same cluster.\n",
        "2. **`MinPts` (Minimum Points):** This is the minimum number of data points required to form a dense region (a core point).\n",
        "\n",
        "They influence clustering by defining density. A larger `eps` or a smaller `MinPts` will result in more points being clustered together, while a smaller `eps` or a larger `MinPts` will result in more points being classified as noise."
      ],
      "metadata": {
        "id": "a_17"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Question 18:** How does K-Means++ improve upon standard K-Means initialization?"
      ],
      "metadata": {
        "id": "q_18"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Answer:**\n",
        "Standard K-Means uses random initialization for its centroids, which can sometimes lead to poor clustering results or slow convergence. **K-Means++** improves this with a smarter initialization technique. It selects the initial centroids to be far away from each other, which helps to avoid placing multiple centroids within the same cluster. This generally leads to better final clusters and faster convergence."
      ],
      "metadata": {
        "id": "a_18"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Question 19:** What is agglomerative clustering?"
      ],
      "metadata": {
        "id": "q_19"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Answer:**\n",
        "**Agglomerative clustering** is a type of hierarchical clustering that follows a **\"bottom-up\"** approach. It starts by treating each data point as its own individual cluster. Then, at each step, it merges the two closest clusters based on a chosen linkage criterion. This process is repeated until all data points belong to a single, large cluster."
      ],
      "metadata": {
        "id": "a_19"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Question 20:** What makes Silhouette Score a better metric than just inertia for model evaluation?"
      ],
      "metadata": {
        "id": "q_20"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Answer:**\n",
        "The Silhouette Score is often better because it measures two important aspects of cluster quality: **cohesion** (how close points are within a cluster) and **separation** (how far apart different clusters are).\n",
        "\n",
        "Inertia, on the other hand, only measures cohesion. A major limitation of inertia is that it always decreases as the number of clusters (K) increases, making it hard to identify the optimal K without a heuristic like the elbow method. The Silhouette Score provides a more balanced view and often has a clear peak at the optimal number of clusters."
      ],
      "metadata": {
        "id": "a_20"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Practical Questions"
      ],
      "metadata": {
        "id": "practice_heading"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Question 21:** Generate synthetic data with 4 centers using make_blobs and apply K-Means clustering. Visualize using a scatter plot."
      ],
      "metadata": {
        "id": "q_21"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.datasets import make_blobs\n",
        "from sklearn.cluster import KMeans\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "X, y = make_blobs(n_samples=300, centers=4, cluster_std=0.8, random_state=42)\n",
        "\n",
        "kmeans = KMeans(n_clusters=4, random_state=42)\n",
        "y_kmeans = kmeans.fit_predict(X)\n",
        "\n",
        "plt.figure(figsize=(8, 6))\n",
        "plt.scatter(X[:, 0], X[:, 1], c=y_kmeans, s=50, cmap='viridis')\n",
        "plt.title('K-Means Clustering on Synthetic Blobs')\n",
        "plt.xlabel('Feature 1')\n",
        "plt.ylabel('Feature 2')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "a_21"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Question 22:** Load the Iris dataset and use Agglomerative Clustering to group the data into 3 clusters. Display the first 10 predicted labels."
      ],
      "metadata": {
        "id": "q_22"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.datasets import load_iris\n",
        "from sklearn.cluster import AgglomerativeClustering\n",
        "\n",
        "iris = load_iris()\n",
        "X = iris.data\n",
        "\n",
        "agg_cluster = AgglomerativeClustering(n_clusters=3)\n",
        "y_pred = agg_cluster.fit_predict(X)\n",
        "\n",
        "print(\"First 10 predicted labels:\")\n",
        "print(y_pred[:10])"
      ],
      "metadata": {
        "id": "a_22"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Question 23:** Generate synthetic data using make_moons and apply DBSCAN. Highlight outliers in the plot."
      ],
      "metadata": {
        "id": "q_23"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.datasets import make_moons\n",
        "from sklearn.cluster import DBSCAN\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "\n",
        "X, y = make_moons(n_samples=200, noise=0.1, random_state=42)\n",
        "\n",
        "dbscan = DBSCAN(eps=0.2, min_samples=5)\n",
        "y_db = dbscan.fit_predict(X)\n",
        "\n",
        "plt.figure(figsize=(8, 6))\n",
        "plt.scatter(X[:, 0], X[:, 1], c=y_db, cmap='viridis')\n",
        "\n",
        "# Highlight outliers (label -1)\n",
        "outliers = X[y_db == -1]\n",
        "if outliers.any():\n",
        "    plt.scatter(outliers[:, 0], outliers[:, 1], c='red', s=70, label='Outliers')\n",
        "\n",
        "plt.title('DBSCAN Clustering on Moons Data')\n",
        "plt.legend()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "a_23"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Question 24:** Load the Wine dataset and apply K-Means clustering after standardizing the features. Print the size of each cluster."
      ],
      "metadata": {
        "id": "q_24"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.datasets import load_wine\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.cluster import KMeans\n",
        "import numpy as np\n",
        "\n",
        "wine = load_wine()\n",
        "X = wine.data\n",
        "\n",
        "scaler = StandardScaler()\n",
        "X_scaled = scaler.fit_transform(X)\n",
        "\n",
        "kmeans = KMeans(n_clusters=3, random_state=42)\n",
        "y_kmeans = kmeans.fit_predict(X_scaled)\n",
        "\n",
        "cluster_sizes = np.bincount(y_kmeans)\n",
        "for i, size in enumerate(cluster_sizes):\n",
        "    print(f\"Size of cluster {i}: {size}\")"
      ],
      "metadata": {
        "id": "a_24"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Question 25:** Use make_circles to generate synthetic data and cluster it using DBSCAN. Plot the result."
      ],
      "metadata": {
        "id": "q_25"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.datasets import make_circles\n",
        "from sklearn.cluster import DBSCAN\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "X, y = make_circles(n_samples=500, factor=0.5, noise=0.05, random_state=42)\n",
        "\n",
        "dbscan = DBSCAN(eps=0.1)\n",
        "y_db = dbscan.fit_predict(X)\n",
        "\n",
        "plt.figure(figsize=(8, 6))\n",
        "plt.scatter(X[:, 0], X[:, 1], c=y_db, cmap='plasma')\n",
        "plt.title('DBSCAN on Concentric Circles')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "a_25"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Question 26:** Load the Breast Cancer dataset, apply MinMaxScaler, and use K-Means with 2 clusters. Output the cluster centroids."
      ],
      "metadata": {
        "id": "q_26"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.datasets import load_breast_cancer\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "from sklearn.cluster import KMeans\n",
        "\n",
        "cancer = load_breast_cancer()\n",
        "X = cancer.data\n",
        "\n",
        "scaler = MinMaxScaler()\n",
        "X_scaled = scaler.fit_transform(X)\n",
        "\n",
        "kmeans = KMeans(n_clusters=2, random_state=42)\n",
        "kmeans.fit(X_scaled)\n",
        "\n",
        "print(\"Cluster Centroids:\")\n",
        "print(kmeans.cluster_centers_)"
      ],
      "metadata": {
        "id": "a_26"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Question 27:** Generate synthetic data using make_blobs with varying cluster standard deviations and cluster with DBSCAN."
      ],
      "metadata": {
        "id": "q_27"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.datasets import make_blobs\n",
        "from sklearn.cluster import DBSCAN\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "X, y = make_blobs(n_samples=200, centers=3, cluster_std=[1.0, 2.5, 0.5], random_state=42)\n",
        "\n",
        "dbscan = DBSCAN(eps=1.0, min_samples=5)\n",
        "y_db = dbscan.fit_predict(X)\n",
        "\n",
        "plt.figure(figsize=(8, 6))\n",
        "plt.scatter(X[:, 0], X[:, 1], c=y_db, cmap='viridis')\n",
        "plt.title('DBSCAN on Blobs with Varying Density')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "a_27"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Question 28:** Load the Digits dataset, reduce it to 2D using PCA, and visualize clusters from K-Means."
      ],
      "metadata": {
        "id": "q_28"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.datasets import load_digits\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.decomposition import PCA\n",
        "from sklearn.cluster import KMeans\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "digits = load_digits()\n",
        "X = digits.data\n",
        "\n",
        "X_scaled = StandardScaler().fit_transform(X)\n",
        "pca = PCA(n_components=2)\n",
        "X_pca = pca.fit_transform(X_scaled)\n",
        "\n",
        "kmeans = KMeans(n_clusters=10, random_state=42)\n",
        "y_kmeans = kmeans.fit_predict(X_pca)\n",
        "\n",
        "plt.figure(figsize=(10, 8))\n",
        "plt.scatter(X_pca[:, 0], X_pca[:, 1], c=y_kmeans, s=20, cmap='viridis')\n",
        "plt.title('K-Means Clustering on Digits Dataset (after PCA)')\n",
        "plt.xlabel('Principal Component 1')\n",
        "plt.ylabel('Principal Component 2')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "a_28"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Question 29:** Create synthetic data using make_blobs and evaluate silhouette scores for k=2 to 5. Display as a bar chart."
      ],
      "metadata": {
        "id": "q_29"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.datasets import make_blobs\n",
        "from sklearn.cluster import KMeans\n",
        "from sklearn.metrics import silhouette_score\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "X, y = make_blobs(n_samples=500, centers=4, cluster_std=1.0, random_state=42)\n",
        "\n",
        "k_range = range(2, 6)\n",
        "silhouette_scores = []\n",
        "\n",
        "for k in k_range:\n",
        "    kmeans = KMeans(n_clusters=k, random_state=42)\n",
        "    y_kmeans = kmeans.fit_predict(X)\n",
        "    score = silhouette_score(X, y_kmeans)\n",
        "    silhouette_scores.append(score)\n",
        "\n",
        "plt.figure(figsize=(8, 5))\n",
        "plt.bar([str(k) for k in k_range], silhouette_scores, color='skyblue')\n",
        "plt.title('Silhouette Scores for Different Values of K')\n",
        "plt.xlabel('Number of Clusters (K)')\n",
        "plt.ylabel('Silhouette Score')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "a_29"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Question 30:** Load the Iris dataset and use hierarchical clustering to group data. Plot a dendrogram with average linkage."
      ],
      "metadata": {
        "id": "q_30"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.datasets import load_iris\n",
        "from scipy.cluster.hierarchy import dendrogram, linkage\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "iris = load_iris()\n",
        "X = iris.data\n",
        "\n",
        "linked = linkage(X, 'average')\n",
        "\n",
        "plt.figure(figsize=(12, 8))\n",
        "dendrogram(linked, orientation='top', distance_sort='descending', show_leaf_counts=True)\n",
        "plt.title('Hierarchical Clustering Dendrogram (Average Linkage)')\n",
        "plt.xlabel('Sample Index')\n",
        "plt.ylabel('Distance')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "a_30"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Question 31:** Generate synthetic data with overlapping clusters using make_blobs, then apply K-Means and visualize with decision boundaries."
      ],
      "metadata": {
        "id": "q_31"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.datasets import make_blobs\n",
        "from sklearn.cluster import KMeans\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "\n",
        "X, y = make_blobs(n_samples=200, centers=3, cluster_std=1.5, random_state=42)\n",
        "\n",
        "kmeans = KMeans(n_clusters=3, random_state=42)\n",
        "kmeans.fit(X)\n",
        "\n",
        "# Plotting decision boundaries\n",
        "x_min, x_max = X[:, 0].min() - 1, X[:, 0].max() + 1\n",
        "y_min, y_max = X[:, 1].min() - 1, X[:, 1].max() + 1\n",
        "xx, yy = np.meshgrid(np.arange(x_min, x_max, 0.02), np.arange(y_min, y_max, 0.02))\n",
        "Z = kmeans.predict(np.c_[xx.ravel(), yy.ravel()])\n",
        "Z = Z.reshape(xx.shape)\n",
        "\n",
        "plt.figure(figsize=(10, 8))\n",
        "plt.imshow(Z, interpolation='nearest', extent=(xx.min(), xx.max(), yy.min(), yy.max()), cmap=plt.cm.Pastel2, aspect='auto', origin='lower')\n",
        "plt.scatter(X[:, 0], X[:, 1], c=y, s=30, cmap='viridis')\n",
        "plt.scatter(kmeans.cluster_centers_[:, 0], kmeans.cluster_centers_[:, 1], s=250, marker='*', c='red', label='Centroids')\n",
        "plt.title('K-Means on Overlapping Clusters')\n",
        "plt.legend()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "a_31"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Question 32:** Load the Digits dataset and apply DBSCAN after reducing dimensions with t-SNE. Visualize the results."
      ],
      "metadata": {
        "id": "q_32"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.datasets import load_digits\n",
        "from sklearn.manifold import TSNE\n",
        "from sklearn.cluster import DBSCAN\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "digits = load_digits()\n",
        "X = digits.data\n",
        "\n",
        "tsne = TSNE(n_components=2, random_state=42)\n",
        "X_tsne = tsne.fit_transform(X)\n",
        "\n",
        "dbscan = DBSCAN(eps=5, min_samples=5)\n",
        "y_db = dbscan.fit_predict(X_tsne)\n",
        "\n",
        "plt.figure(figsize=(10, 8))\n",
        "plt.scatter(X_tsne[:, 0], X_tsne[:, 1], c=y_db, cmap='viridis')\n",
        "plt.title('DBSCAN on Digits Dataset (after t-SNE)')\n",
        "plt.xlabel('t-SNE Component 1')\n",
        "plt.ylabel('t-SNE Component 2')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "a_32"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Question 33:** Generate synthetic data using make_blobs and apply Agglomerative Clustering with complete linkage. Plot the result."
      ],
      "metadata": {
        "id": "q_33"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.datasets import make_blobs\n",
        "from sklearn.cluster import AgglomerativeClustering\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "X, y = make_blobs(n_samples=200, centers=4, cluster_std=0.7, random_state=42)\n",
        "\n",
        "agg_cluster = AgglomerativeClustering(n_clusters=4, linkage='complete')\n",
        "y_pred = agg_cluster.fit_predict(X)\n",
        "\n",
        "plt.figure(figsize=(8, 6))\n",
        "plt.scatter(X[:, 0], X[:, 1], c=y_pred, cmap='viridis')\n",
        "plt.title('Agglomerative Clustering (Complete Linkage)')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "a_33"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Question 34:** Load the Breast Cancer dataset and compare inertia values for K=2 to 6 using K-Means. Show results in a line plot."
      ],
      "metadata": {
        "id": "q_34"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.datasets import load_breast_cancer\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.cluster import KMeans\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "cancer = load_breast_cancer()\n",
        "X = cancer.data\n",
        "X_scaled = StandardScaler().fit_transform(X)\n",
        "\n",
        "k_range = range(2, 7)\n",
        "inertias = []\n",
        "\n",
        "for k in k_range:\n",
        "    kmeans = KMeans(n_clusters=k, random_state=42)\n",
        "    kmeans.fit(X_scaled)\n",
        "    inertias.append(kmeans.inertia_)\n",
        "\n",
        "plt.figure(figsize=(8, 5))\n",
        "plt.plot(k_range, inertias, marker='o')\n",
        "plt.title('Elbow Method for Optimal K')\n",
        "plt.xlabel('Number of Clusters (K)')\n",
        "plt.ylabel('Inertia')\n",
        "plt.xticks(k_range)\n",
        "plt.grid(True)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "a_34"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Question 35:** Generate synthetic concentric circles using make_circles and cluster using Agglomerative Clustering with single linkage."
      ],
      "metadata": {
        "id": "q_35"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.datasets import make_circles\n",
        "from sklearn.cluster import AgglomerativeClustering\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "X, y = make_circles(n_samples=500, factor=0.5, noise=0.05, random_state=42)\n",
        "\n",
        "agg_cluster = AgglomerativeClustering(n_clusters=2, linkage='single')\n",
        "y_pred = agg_cluster.fit_predict(X)\n",
        "\n",
        "plt.figure(figsize=(8, 6))\n",
        "plt.scatter(X[:, 0], X[:, 1], c=y_pred, cmap='plasma')\n",
        "plt.title('Agglomerative Clustering (Single Linkage) on Circles')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "a_35"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Question 36:** Use the Wine dataset, apply DBSCAN after scaling the data, and count the number of clusters (excluding noise)."
      ],
      "metadata": {
        "id": "q_36"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.datasets import load_wine\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.cluster import DBSCAN\n",
        "import numpy as np\n",
        "\n",
        "wine = load_wine()\n",
        "X = wine.data\n",
        "X_scaled = StandardScaler().fit_transform(X)\n",
        "\n",
        "dbscan = DBSCAN(eps=2.5, min_samples=5)\n",
        "y_db = dbscan.fit_predict(X_scaled)\n",
        "\n",
        "n_clusters = len(np.unique(y_db[y_db != -1]))\n",
        "print(f\"Number of clusters found by DBSCAN (excluding noise): {n_clusters}\")"
      ],
      "metadata": {
        "id": "a_36"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Question 37:** Generate synthetic data with make_blobs and apply KMeans. Then plot the cluster centers on top of the data points."
      ],
      "metadata": {
        "id": "q_37"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.datasets import make_blobs\n",
        "from sklearn.cluster import KMeans\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "X, y = make_blobs(n_samples=300, centers=4, cluster_std=0.8, random_state=42)\n",
        "\n",
        "kmeans = KMeans(n_clusters=4, random_state=42)\n",
        "y_kmeans = kmeans.fit_predict(X)\n",
        "\n",
        "plt.figure(figsize=(8, 6))\n",
        "plt.scatter(X[:, 0], X[:, 1], c=y_kmeans, s=50, cmap='viridis')\n",
        "plt.scatter(kmeans.cluster_centers_[:, 0], kmeans.cluster_centers_[:, 1], s=250, marker='*', c='red', label='Centroids')\n",
        "plt.title('K-Means Clustering with Centroids')\n",
        "plt.legend()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "a_37"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Question 38:** Load the Iris dataset, cluster with DBSCAN, and print how many samples were identified as noise."
      ],
      "metadata": {
        "id": "q_38"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.datasets import load_iris\n",
        "from sklearn.cluster import DBSCAN\n",
        "import numpy as np\n",
        "\n",
        "iris = load_iris()\n",
        "X = iris.data\n",
        "\n",
        "dbscan = DBSCAN(eps=0.5, min_samples=5)\n",
        "y_db = dbscan.fit_predict(X)\n",
        "\n",
        "n_noise = np.sum(y_db == -1)\n",
        "print(f\"Number of noise points identified by DBSCAN: {n_noise}\")"
      ],
      "metadata": {
        "id": "a_38"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Question 39:** Generate synthetic non-linearly separable data using make_moons, apply K-Means, and visualize the clustering result."
      ],
      "metadata": {
        "id": "q_39"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.datasets import make_moons\n",
        "from sklearn.cluster import KMeans\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "X, y = make_moons(n_samples=200, noise=0.05, random_state=42)\n",
        "\n",
        "kmeans = KMeans(n_clusters=2, random_state=42)\n",
        "y_kmeans = kmeans.fit_predict(X)\n",
        "\n",
        "plt.figure(figsize=(8, 6))\n",
        "plt.scatter(X[:, 0], X[:, 1], c=y_kmeans, cmap='viridis')\n",
        "plt.title('K-Means on Non-Linearly Separable Data (Fails)')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "a_39"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Question 40:** Load the Digits dataset, apply PCA to reduce to 3 components, then use KMeans and visualize with a 3D scatter plot."
      ],
      "metadata": {
        "id": "q_40"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.datasets import load_digits\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.decomposition import PCA\n",
        "from sklearn.cluster import KMeans\n",
        "import matplotlib.pyplot as plt\n",
        "from mpl_toolkits.mplot3d import Axes3D\n",
        "\n",
        "digits = load_digits()\n",
        "X = digits.data\n",
        "\n",
        "X_scaled = StandardScaler().fit_transform(X)\n",
        "pca = PCA(n_components=3)\n",
        "X_pca = pca.fit_transform(X_scaled)\n",
        "\n",
        "kmeans = KMeans(n_clusters=10, random_state=42)\n",
        "y_kmeans = kmeans.fit_predict(X_pca)\n",
        "\n",
        "fig = plt.figure(figsize=(10, 8))\n",
        "ax = fig.add_subplot(111, projection='3d')\n",
        "ax.scatter(X_pca[:, 0], X_pca[:, 1], X_pca[:, 2], c=y_kmeans, cmap='viridis')\n",
        "ax.set_title('K-Means on Digits Dataset (3D PCA)')\n",
        "ax.set_xlabel('PC 1')\n",
        "ax.set_ylabel('PC 2')\n",
        "ax.set_zlabel('PC 3')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "a_40"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Question 41:** Generate synthetic blobs with 5 centers and apply KMeans. Then use silhouette_score to evaluate the clustering."
      ],
      "metadata": {
        "id": "q_41"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.datasets import make_blobs\n",
        "from sklearn.cluster import KMeans\n",
        "from sklearn.metrics import silhouette_score\n",
        "\n",
        "X, y = make_blobs(n_samples=500, centers=5, cluster_std=1.0, random_state=42)\n",
        "\n",
        "kmeans = KMeans(n_clusters=5, random_state=42)\n",
        "y_kmeans = kmeans.fit_predict(X)\n",
        "\n",
        "score = silhouette_score(X, y_kmeans)\n",
        "print(f\"Silhouette Score for K-Means (k=5): {score:.4f}\")"
      ],
      "metadata": {
        "id": "a_41"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Question 42:** Load the Breast Cancer dataset, reduce dimensionality using PCA, and apply Agglomerative Clustering. Visualize in 2D."
      ],
      "metadata": {
        "id": "q_42"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.datasets import load_breast_cancer\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.decomposition import PCA\n",
        "from sklearn.cluster import AgglomerativeClustering\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "cancer = load_breast_cancer()\n",
        "X = cancer.data\n",
        "\n",
        "X_scaled = StandardScaler().fit_transform(X)\n",
        "pca = PCA(n_components=2)\n",
        "X_pca = pca.fit_transform(X_scaled)\n",
        "\n",
        "agg_cluster = AgglomerativeClustering(n_clusters=2)\n",
        "y_pred = agg_cluster.fit_predict(X_pca)\n",
        "\n",
        "plt.figure(figsize=(8, 6))\n",
        "plt.scatter(X_pca[:, 0], X_pca[:, 1], c=y_pred, cmap='viridis')\n",
        "plt.title('Agglomerative Clustering on Breast Cancer Data (after PCA)')\n",
        "plt.xlabel('Principal Component 1')\n",
        "plt.ylabel('Principal Component 2')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "a_42"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Question 43:** Generate noisy circular data using make_circles and visualize clustering results from KMeans and DBSCAN side-by-side."
      ],
      "metadata": {
        "id": "q_43"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.datasets import make_circles\n",
        "from sklearn.cluster import KMeans, DBSCAN\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "X, y = make_circles(n_samples=500, factor=0.5, noise=0.08, random_state=42)\n",
        "\n",
        "y_kmeans = KMeans(n_clusters=2, random_state=42).fit_predict(X)\n",
        "y_dbscan = DBSCAN(eps=0.15).fit_predict(X)\n",
        "\n",
        "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(14, 6))\n",
        "\n",
        "ax1.scatter(X[:, 0], X[:, 1], c=y_kmeans, cmap='plasma')\n",
        "ax1.set_title('K-Means Result')\n",
        "\n",
        "ax2.scatter(X[:, 0], X[:, 1], c=y_dbscan, cmap='plasma')\n",
        "ax2.set_title('DBSCAN Result')\n",
        "\n",
        "plt.suptitle('K-Means vs. DBSCAN on Noisy Circles')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "a_43"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Question 44:** Load the Iris dataset and plot the Silhouette Coefficient for each sample after KMeans clustering."
      ],
      "metadata": {
        "id": "q_44"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.datasets import load_iris\n",
        "from sklearn.cluster import KMeans\n",
        "from sklearn.metrics import silhouette_samples\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "\n",
        "iris = load_iris()\n",
        "X = iris.data\n",
        "\n",
        "kmeans = KMeans(n_clusters=3, random_state=42)\n",
        "y_kmeans = kmeans.fit_predict(X)\n",
        "\n",
        "sample_silhouette_values = silhouette_samples(X, y_kmeans)\n",
        "\n",
        "plt.figure(figsize=(10, 6))\n",
        "y_ax_lower = 10\n",
        "for i in range(3):\n",
        "    ith_cluster_silhouette_values = sample_silhouette_values[y_kmeans == i]\n",
        "    ith_cluster_silhouette_values.sort()\n",
        "    size_cluster_i = ith_cluster_silhouette_values.shape[0]\n",
        "    y_ax_upper = y_ax_lower + size_cluster_i\n",
        "    plt.fill_betweenx(np.arange(y_ax_lower, y_ax_upper), 0, ith_cluster_silhouette_values)\n",
        "    y_ax_lower = y_ax_upper + 10\n",
        "\n",
        "plt.title('Silhouette Plot for each Sample')\n",
        "plt.xlabel('Silhouette Coefficient')\n",
        "plt.ylabel('Cluster Label')\n",
        "plt.yticks([])\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "a_44"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Question 45:** Generate synthetic data using make_blobs and apply Agglomerative Clustering with 'average' linkage. Visualize clusters."
      ],
      "metadata": {
        "id": "q_45"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.datasets import make_blobs\n",
        "from sklearn.cluster import AgglomerativeClustering\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "X, y = make_blobs(n_samples=200, centers=4, cluster_std=0.9, random_state=42)\n",
        "\n",
        "agg_cluster = AgglomerativeClustering(n_clusters=4, linkage='average')\n",
        "y_pred = agg_cluster.fit_predict(X)\n",
        "\n",
        "plt.figure(figsize=(8, 6))\n",
        "plt.scatter(X[:, 0], X[:, 1], c=y_pred, cmap='viridis')\n",
        "plt.title('Agglomerative Clustering (Average Linkage)')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "a_45"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Question 46:** Load the Wine dataset, apply KMeans, and visualize the cluster assignments in a seaborn pairplot (first 4 features)."
      ],
      "metadata": {
        "id": "q_46"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.datasets import load_wine\n",
        "from sklearn.cluster import KMeans\n",
        "import pandas as pd\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "wine = load_wine()\n",
        "X = wine.data\n",
        "\n",
        "kmeans = KMeans(n_clusters=3, random_state=42)\n",
        "y_kmeans = kmeans.fit_predict(X)\n",
        "\n",
        "df = pd.DataFrame(X[:, :4], columns=wine.feature_names[:4])\n",
        "df['cluster'] = y_kmeans\n",
        "\n",
        "sns.pairplot(df, hue='cluster', palette='viridis')\n",
        "plt.suptitle('K-Means Clusters on Wine Dataset (First 4 Features)', y=1.02)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "a_46"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Question 47:** Generate noisy blobs using make_blobs and use DBSCAN to identify both clusters and noise points. Print the count."
      ],
      "metadata": {
        "id": "q_47"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.datasets import make_blobs\n",
        "from sklearn.cluster import DBSCAN\n",
        "import numpy as np\n",
        "\n",
        "X, y = make_blobs(n_samples=250, centers=3, cluster_std=1.2, random_state=42)\n",
        "\n",
        "dbscan = DBSCAN(eps=1.0, min_samples=5)\n",
        "y_db = dbscan.fit_predict(X)\n",
        "\n",
        "n_clusters = len(np.unique(y_db[y_db != -1]))\n",
        "n_noise = np.sum(y_db == -1)\n",
        "\n",
        "print(f\"Number of clusters found: {n_clusters}\")\n",
        "print(f\"Number of noise points found: {n_noise}\")"
      ],
      "metadata": {
        "id": "a_47"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Question 48:** Load the Digits dataset, reduce dimensions using t-SNE, then apply Agglomerative Clustering and plot the clusters."
      ],
      "metadata": {
        "id": "q_48"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.datasets import load_digits\n",
        "from sklearn.manifold import TSNE\n",
        "from sklearn.cluster import AgglomerativeClustering\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "digits = load_digits()\n",
        "X = digits.data\n",
        "\n",
        "tsne = TSNE(n_components=2, random_state=42)\n",
        "X_tsne = tsne.fit_transform(X)\n",
        "\n",
        "agg_cluster = AgglomerativeClustering(n_clusters=10)\n",
        "y_pred = agg_cluster.fit_predict(X_tsne)\n",
        "\n",
        "plt.figure(figsize=(10, 8))\n",
        "plt.scatter(X_tsne[:, 0], X_tsne[:, 1], c=y_pred, cmap='viridis')\n",
        "plt.title('Agglomerative Clustering on Digits Dataset (after t-SNE)')\n",
        "plt.xlabel('t-SNE Component 1')\n",
        "plt.ylabel('t-SNE Component 2')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "a_48"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}